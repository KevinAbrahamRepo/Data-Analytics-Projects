{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author - Kevin Abraham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary python libraries/pacakges\n",
    "import re\n",
    "#import tweepy\n",
    "#from tweepy.streaming import StreamListener\n",
    "#from tweepy import OAuthHandler\n",
    "#from tweepy import Stream\n",
    "from textblob import TextBlob\n",
    "import csv \n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "import os,sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_api():\n",
    "    # Function that loads the twitter API after authorizing the user\n",
    "\n",
    "    access_token = \"1058460791xxxxxxxxxxxxxxx\"\n",
    "    access_token_secret = \"1058460791xxxxxxxxxxxxxxx\"\n",
    "    consumer_key = \"1058460791xxxxxxxxxxxxxxx\"\n",
    "    consumer_secret = \"1058460791xxxxxxxxxxxxxxx\"\n",
    "\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    # load the twitter API via tweepy\n",
    "    return tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_search(api, query, max_tweets, max_id, since_id, geocode):\n",
    "    #  Function that takes in a search string 'query', the maximum\n",
    "    #  number of tweets 'max_tweets', and the minimum (i.e., starting)\n",
    "    #  tweet id. It returns a list of tweepy.models.Status objects. '''\n",
    "\n",
    "    searched_tweets = []\n",
    "    while len(searched_tweets) < max_tweets:\n",
    "        remaining_tweets = max_tweets - len(searched_tweets)\n",
    "        try:\n",
    "            new_tweets = api.search(q=query, count=remaining_tweets, since_id=str(since_id), max_id=str(max_id-1))\n",
    "#                                   geocode=geocode)\n",
    "            print('found',len(new_tweets),'tweets')\n",
    "            if not new_tweets:\n",
    "                print('no tweets found')\n",
    "                break\n",
    "            searched_tweets.extend(new_tweets)\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError:\n",
    "            print('exception raised, waiting 15 minutes')\n",
    "            print('(until:', dt.datetime.now()+dt.timedelta(minutes=15), ')')\n",
    "            time.sleep(15*60)\n",
    "            break # stop the loop\n",
    "    return searched_tweets, max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_id(api, date='', days_ago=9, query='a'):\n",
    "     #   Function that gets the ID of a tweet. This ID can then be\n",
    "     #   used as a 'starting point' from which to search. The query is\n",
    "     #   required and has been set to a commonly used word by default.\n",
    "     #   The variable 'days_ago' has been initialized to the maximum\n",
    "     #   amount we are able to search back in time (9)\n",
    "\n",
    "    if date:\n",
    "        # return an ID from the start of the given day\n",
    "        td = date + dt.timedelta(days=1)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        tweet = api.search(q=query, count=1, until=tweet_date)\n",
    "    else:\n",
    "        # return an ID from __ days ago\n",
    "        td = dt.datetime.now() - dt.timedelta(days=days_ago)\n",
    "        tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)\n",
    "        # get list of up to 10 tweets\n",
    "        tweet = api.search(q=query, count=10, until=tweet_date)\n",
    "        print('search limit (start/stop):',tweet[0].created_at)\n",
    "        # return the id of the first tweet in the list\n",
    "        return tweet[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tweets(tweets, filename):\n",
    "    # Function that appends tweets to a file. '''\n",
    "\n",
    "    with open(filename, 'a') as f:\n",
    "        for tweet in tweets:\n",
    "            json.dump(tweet._json, f)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #    This is a script that continuously searches for tweets\n",
    "    #    that were created over a given number of days. The search\n",
    "    #    dates and search phrase can be changed below. '''\n",
    "\n",
    "\n",
    "\n",
    "    # search variables: \n",
    "    search_phrases = ['#GreyCup','Grey Cup', 'GreyCup']\n",
    "    time_limit = 1.5                           # runtime limit in hours\n",
    "    max_tweets = 100                           # number of tweets per search (will be\n",
    "                                               # iterated over) - maximum is 100\n",
    "    min_days_old, max_days_old = 1, 9          # search limits e.g., from 1 to 9 days old\n",
    "                                               # gives current weekday from last week,\n",
    "                                               # min_days_old=0 will search from right now\n",
    "    USA = '39.8,-95.583068847656,2500km'       # this geocode includes nearly all American\n",
    "                                               # states (and a large portion of Canada)\n",
    "    \n",
    "\n",
    "    # loop over search items,\n",
    "    # creating a new file for each\n",
    "    for search_phrase in search_phrases:\n",
    "\n",
    "        print('Search phrase =', search_phrase)\n",
    "\n",
    "        # other variables \n",
    "        name = search_phrase.split()[0]\n",
    "        json_file_root = name + '/'  + name\n",
    "        os.makedirs(os.path.dirname(json_file_root), exist_ok=True)\n",
    "        read_IDs = False\n",
    "        \n",
    "        # open a file in which to store the tweets\n",
    "        if max_days_old - min_days_old == 1:\n",
    "            d = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "            day = '{0}-{1:0>2}-{2:0>2}'.format(d.year, d.month, d.day)\n",
    "        else:\n",
    "            d1 = dt.datetime.now() - dt.timedelta(days=max_days_old-1)\n",
    "            d2 = dt.datetime.now() - dt.timedelta(days=min_days_old)\n",
    "            day = '{0}-{1:0>2}-{2:0>2}_to_{3}-{4:0>2}-{5:0>2}'.format(\n",
    "                  d1.year, d1.month, d1.day, d2.year, d2.month, d2.day)\n",
    "        json_file = json_file_root + '_' + day + '.json'\n",
    "        if os.path.isfile(json_file):\n",
    "            print('Appending tweets to file named: ',json_file)\n",
    "            read_IDs = True\n",
    "        \n",
    "        # authorize and load the twitter API\n",
    "        api = load_api()\n",
    "        \n",
    "        # set the 'starting point' ID for tweet collection\n",
    "        if read_IDs:\n",
    "            # open the json file and get the latest tweet ID\n",
    "            with open(json_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                max_id = json.loads(lines[-1])['id']\n",
    "                print('Searching from the bottom ID in file')\n",
    "        else:\n",
    "            # get the ID of a tweet that is min_days_old\n",
    "            if min_days_old == 0:\n",
    "                max_id = -1\n",
    "            else:\n",
    "                max_id = get_tweet_id(api, days_ago=(min_days_old-1))\n",
    "        # set the smallest ID to search for\n",
    "        since_id = get_tweet_id(api, days_ago=(max_days_old-1))\n",
    "        print('max id (starting point) =', max_id)\n",
    "        print('since id (ending point) =', since_id)\n",
    "        \n",
    "\n",
    "\n",
    "        # tweet gathering loop  '''\n",
    "        start = dt.datetime.now()\n",
    "        end = start + dt.timedelta(hours=time_limit)\n",
    "        count, exitcount = 0, 0\n",
    "        while dt.datetime.now() < end:\n",
    "            count += 1\n",
    "            print('count =',count)\n",
    "            # collect tweets and update max_id\n",
    "            tweets, max_id = tweet_search(api, search_phrase, max_tweets,\n",
    "                                          max_id=max_id, since_id=since_id,\n",
    "                                          geocode=USA)\n",
    "            # write tweets to file in JSON format\n",
    "            if tweets:\n",
    "                write_tweets(tweets, json_file)\n",
    "                exitcount = 0\n",
    "            else:\n",
    "                exitcount += 1\n",
    "                if exitcount == 3:\n",
    "                    if search_phrase == search_phrases[-1]:\n",
    "                        sys.exit('Maximum number of empty tweet strings reached - exiting')\n",
    "                    else:\n",
    "                        print('Maximum number of empty tweet strings reached - breaking')\n",
    "                        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run program from MAIN (starting point)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_tweet_df(tweets):\n",
    "    df = pd.DataFrame()\n",
    " \n",
    "    df['text'] = list(map(lambda tweet: tweet['text'], tweets))\n",
    " \n",
    "    df['location'] = list(map(lambda tweet: tweet['user']['location'], tweets))\n",
    "    \n",
    "    df['lang'] = list(map(lambda tweet: tweet['lang'], tweets))\n",
    " \n",
    "    df['country_code'] = list(map(lambda tweet: tweet['place']['country_code']\n",
    "                                  if tweet['place'] != None else None, tweets))\n",
    " \n",
    "    df['long'] = list(map(lambda tweet: tweet['coordinates']['coordinates'][0]\n",
    "                        if tweet['coordinates'] != None else None, tweets))\n",
    " \n",
    "    df['latt'] = list(map(lambda tweet: tweet['coordinates']['coordinates'][1]\n",
    "                        if tweet['coordinates'] != None else None, tweets))\n",
    " \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local path for tweets collected for #GreyCup & Grey Cup\n",
    "local_path_1='C:/Users/Owner/Desktop/2018 - Mcmaster Data Analytics App/Courses/Fall 2018/BDA 102/Table of Contents/Lab Assignments/Project Files/Files From VM/GreyCup - Project Files/#GreyCup/#GreyCup_2018-11-21_to_2018-11-28.json'\n",
    "local_path_2='C:/Users/Owner/Desktop/2018 - Mcmaster Data Analytics App/Courses/Fall 2018/BDA 102/Table of Contents/Lab Assignments/Project Files/Files From VM/GreyCup - Project Files/Grey/Grey_2018-11-21_to_2018-11-28.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the jason files into tweets list\n",
    "\n",
    "tweet_files = [local_path_1, local_path_2]\n",
    "tweets = []\n",
    "for file in tweet_files:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            tweets.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greycup_tweets = populate_tweet_df(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(greycup_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greycup_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greycup_tweets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will require you to install pyproj and basemap\n",
    "# Follow instructions here --> https://www.lfd.uci.edu/~gohlke/pythonlibs/\n",
    "# Skip down to 'Basemap' title and install latest version 37/64bit OS pyproj + basemap\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the blank world map\n",
    "my_map = Basemap(projection='merc', lat_0=50, lon_0=-100,\n",
    "                     resolution = 'h', area_thresh = 5000.0,\n",
    "                     llcrnrlon=-140, llcrnrlat=-55,\n",
    "                     urcrnrlon=160, urcrnrlat=70)\n",
    "# set resolution='h' for high quality\n",
    " \n",
    "# draw elements onto the world map\n",
    "my_map.drawcountries()\n",
    "#my_map.drawstates()\n",
    "my_map.drawcoastlines(antialiased=False, linewidth=0.005)\n",
    " \n",
    "# add coordinates as red dots\n",
    "longs = list(greycup_tweets.loc[(greycup_tweets.long != None)].long)\n",
    "latts = list(greycup_tweets.loc[greycup_tweets.latt != None].latt)\n",
    "x, y = my_map(longs, latts)\n",
    "my_map.plot(x, y, 'ro', markersize=6, alpha=0.5)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy over greycup_tweets to new dataframe 'tweets' for simplicity\n",
    "tweets = greycup_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count top languages in which tweets were written\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "tweets_by_lang[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graphically top languages in which the tweets were written\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Languages', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 3 languages', fontsize=15, fontweight='bold')\n",
    "tweets_by_lang[:3].plot(ax=ax, kind='bar', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count top 3 user location from which tweets were sent\n",
    "tweets_by_location = tweets['location'].value_counts()\n",
    "tweets_by_location[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top user locations from which the tweets were sent\n",
    "tweets_by_location = tweets['location'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Countries', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 3 User Locations', fontsize=15, fontweight='bold')\n",
    "tweets_by_location[2:5].plot(ax=ax, kind='bar', color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count top 2 countries from which tweets were sent\n",
    "tweets_by_country = tweets['country_code'].value_counts()\n",
    "tweets_by_country[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top countries from which the tweets were sent\n",
    "tweets_by_country = tweets['country_code'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Countries', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 2 Countries', fontsize=15, fontweight='bold')\n",
    "tweets_by_country[0:2].plot(ax=ax, kind='bar', color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to return boolean result if a word is found in the text\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "    if match:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional columns to the 'tweets' data frame:\n",
    "\n",
    "# Begin sorting results by relevancy\n",
    "# This list CAN vary depending on the search 'texts'\n",
    "tweets['#GreyCup'] = tweets['text'].apply(lambda tweet: word_in_text('#GreyCup', tweet))\n",
    "tweets['Grey Cup'] = tweets['text'].apply(lambda tweet: word_in_text('Grey Cup', tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts for each reference of the given topic\n",
    "print (tweets['#GreyCup'].value_counts()[True])\n",
    "print (tweets['Grey Cup'].value_counts()[True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common words before ANY text cleaning\n",
    "# Top 5\n",
    "freq = pd.Series(' '.join(tweets['text']).split()).value_counts()[:5]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only extract relevant tweets containing useful keywords\n",
    "\n",
    "# tweets.drop(['Unnamed: 5', 'Unnamed: 6', 'sepal length.1', 'Unnamed: 8'],axis=1,inplace=True)\n",
    "tweets['Relevant'] = tweets['text'].apply(lambda tweet: word_in_text('#GreyCup', tweet) or word_in_text('Grey Cup', tweet))\n",
    "\n",
    "# Print count for of relevant tweets\n",
    "print ('Relevant Tweets: ' + str(tweets['Relevant'].value_counts()[True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use function if required to clean text\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove hashtags, URLs, mentions, punctuations, RTs, whitespace \n",
    "def clean_tweet(tweet):\n",
    "    \n",
    "    #tweet = re.sub('http\\S+\\s*', '', tweet)  # remove URLs\n",
    "    #tweet = re.sub('RT|cc', '', tweet)       # remove RT and cc\n",
    "    #tweet = re.sub('RT', '', tweet)          # remove RT only\n",
    "    #tweet = re.sub('#\\S+', '', tweet)        # remove hashtags\n",
    "    #tweet = re.sub('@\\S+', '', tweet)        # remove mentions    \n",
    "    #tweet = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', tweet) # remove punctuations     \n",
    "    #tweet = re.sub('\\s+', ' ', tweet)        # remove extra whitespace\n",
    "    #tweet = remove_emoji(tweet)              # remove any emoticons/images/symbols/flags/pics\n",
    "    \n",
    "    # [Do not REMOVE numbers - will need all references to 2018 in text]\n",
    "    #tweet = re.sub('[0-9_]', '', tweet)      # remove numbers \n",
    "    \n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|(RT|cc)\", \" \", tweet).split()) \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold length of tweets dataframe\n",
    "length = len(tweets)\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through every 'text' record and clean tweets\n",
    "\n",
    "i=0\n",
    "\n",
    "for tweet in tweets['text']:\n",
    "    tweet = clean_tweet(tweet).lower()\n",
    "    tweets.loc[i,'text'] = tweet\n",
    "    if(i==length):\n",
    "        break\n",
    "    else:\n",
    "        i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate heat map of tweets dataframe for any column(s) with NaNs/None/Nulls\n",
    "import seaborn as sns\n",
    "sns.heatmap(tweets.isnull(),yticklabels=False,cbar=False,cmap='gist_rainbow_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total # of NaNs/NULL value for each Columns/Records in data frame\n",
    "\n",
    "null_columns=tweets.columns[tweets.isnull().any()]\n",
    "tweets[null_columns].isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column(s) if majori-ty are None/Null/NaNs\n",
    "# NOT Required anymore\n",
    "tweets.drop(['country_code','long','latt'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common words AFTER cleaning text (No STOPWORDS removed)\n",
    "# Top 5\n",
    "freq = pd.Series(' '.join(tweets['text']).split()).value_counts()[:5]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFter tweets have been cleaned of hashtags, URLs, mentions, punctuations, RTs, whitespace.\n",
    "# Perform sentiment analysis\n",
    "\n",
    "def get_tweet_sentiment(tweet): \n",
    "\n",
    "    #Utility function to classify sentiment of passed tweet \n",
    "    #using textblob's sentiment method \n",
    "\n",
    "    # create TextBlob object of passed tweet text \n",
    "    analysis = TextBlob(clean_tweet(tweet)) \n",
    "    # set sentiment \n",
    "    if analysis.sentiment.polarity > 0: \n",
    "        return 4  #positive\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        return 2  #neutral\n",
    "    else: \n",
    "        return 0  #negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column 'sentiment' --> Class attribute for sentiment analysis\n",
    "tweets['sentiment'] = tweets['text'].apply(lambda tweet: get_tweet_sentiment(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "# percentage of positive tweets \n",
    "ptweets = [tweet for tweet in tweets['sentiment'] if tweet == 4] \n",
    "print(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets))) \n",
    "\n",
    "# percentage of negative tweets \n",
    "ntweets = [tweet for tweet in tweets['sentiment'] if tweet == 0] \n",
    "print(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets))) \n",
    "\n",
    "# percentage of neutral tweets \n",
    "neutweets = [tweet for tweet in tweets['sentiment'] if tweet == 2] \n",
    "print(\"Neutral tweets percentage: {} %\".format(100*len(neutweets)/len(tweets))) \n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of sentiments of the analysed tweets\n",
    "\n",
    "prg_langs = ['Positive', 'Negative', 'Neutral']\n",
    "tweets_by_prg_lang = [len(ptweets), len(ntweets), len(neutweets)]\n",
    "\n",
    "x_pos = list(range(3))\n",
    "width = 0.8\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(x_pos, tweets_by_prg_lang, width, alpha=1, color='y')\n",
    "\n",
    "# Setting axis labels and ticks\n",
    "ax.set_ylabel('Number of tweets', fontsize=15)\n",
    "ax.set_title('Ranking: Positive vs Negative vs Neutral', fontsize=10, fontweight='bold')\n",
    "ax.set_xticks([p + 0.4 * width for p in x_pos])\n",
    "ax.set_xticklabels(prg_langs)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download these packages if required\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "# lst=LancasterStemmer() # more aggressive - so avoid using this stemming technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(words):\n",
    "    stemmer = PorterStemmer() #not as aggressive\n",
    "    stems=[]\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and stem results\n",
    "i=0\n",
    "tokenized_tweet = []\n",
    "\n",
    "for tweet in tweets['text']:\n",
    "    tokenized = word_tokenize(tweet)\n",
    "    stems = stem_words(tokenized)\n",
    "    tweets.loc[i,'text'] = \" \".join(list(x for x in stems if x not in stop))\n",
    "    if(i==length):\n",
    "        break\n",
    "    else:\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common words AFTER ALL text cleaning is complete (Stemmed text)\n",
    "# Top 5\n",
    "freq = pd.Series(' '.join(tweets['text']).split()).value_counts()[:4]\n",
    "freq = freq.to_frame()\n",
    "freq.columns = ['count']\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prg_langs = ['greycup', 'grey', 'cup', 'CFL']\n",
    "tweets_by_prg_lang = [freq['count'][0],freq['count'][1], freq['count'][2], freq['count'][3]]\n",
    "\n",
    "x_pos = list(range(4))\n",
    "width = 0.8\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(x_pos, tweets_by_prg_lang, width, alpha=1, color='r')\n",
    "\n",
    "# Setting axis labels and ticks\n",
    "ax.set_ylabel('Number of Occurences', fontsize=15)\n",
    "ax.set_title('- Common Words Ranking -', fontsize=10, fontweight='bold')\n",
    "ax.set_xticks([p + 0.4 * width for p in x_pos])\n",
    "ax.set_xticklabels(prg_langs)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wordcloud - python terminal windoe\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(tweets,col):\n",
    "    #stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(background_color=\"white\",stopwords=stop,random_state = 2016).generate(\" \".join([i for i in tweets[col]]))\n",
    "    plt.figure( figsize=(15,10), facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Good Morning Datascience+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud of most commonly occuring words\n",
    "wordcloud(tweets,'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the data into a training and test set\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class attributes for sentiment analysis\n",
    "# Display last 12\n",
    "tweets['sentiment'].head(n=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate train and test dataset (sentiment - class attribute)\n",
    "X = tweets.drop('sentiment', axis = 1)\n",
    "y = tweets['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 5) # 80% - 20% split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type = Series\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = list(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for train dataset (X_test, y_test)\n",
    "df = pd.DataFrame(vec.fit_transform(X_train).toarray(), columns=vec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for Test data set (X_test, y_test)\n",
    "test_df = pd.DataFrame(vec.transform(X_test).toarray(), columns = vec.get_feature_names())\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = y_train.to_frame()\n",
    "df_train.columns = ['class']\n",
    "\n",
    "df_test = y_test.to_frame()\n",
    "df_test.columns = ['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df\n",
    "y_train = df_train['class']\n",
    "\n",
    "x_test = test_df\n",
    "y_test = df_test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1:\n",
    "# Build Naive_Bayes supervised learning model and compute accuracy against test dataset\n",
    "\n",
    "algorithm_a = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model with train dataset\n",
    "algorithm_a.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = algorithm_a.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model accuracy with Test dataset (class, output)\n",
    "metrics.accuracy_score(y_test, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model predicted confusion matrix \n",
    "metrics.confusion_matrix(y_test,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2:\n",
    "# Build support vector machines (SVM) learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR KERNEL (SVM) Learning Model\n",
    "\n",
    "import sklearn.svm as svm\n",
    "clf = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier \n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs slightly better that Naive_Bayes\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Kernel (SVM)\n",
    "from sklearn.svm import SVC  \n",
    "svclassifier = SVC(kernel='poly', degree=8)  \n",
    "svclassifier.fit(x_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(x_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare confusion matrix results:\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
